{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn\n",
    "\n",
    "action2name = {\n",
    "    0: \"N\",\n",
    "    1: \"W\",\n",
    "    2: \"S\",\n",
    "    3: \"E\",\n",
    "}\n",
    "\n",
    "name2action = {v: k for k, v in action2name.items()}\n",
    "\n",
    "action2delta = {\n",
    "    0: (0, 1),  # x,y format\n",
    "    1: (-1, 0),\n",
    "    2: (0, -1),\n",
    "    3: (1, 0),\n",
    "}\n",
    "\n",
    "delta2action = {v: k for k, v in action2delta.items()}\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_position=[0, 0],\n",
    "        env_shape=(100, 100),\n",
    "        x_bounds=range(5, 95),\n",
    "        y_bounds=range(5, 95),\n",
    "        \n",
    "    ):\n",
    "\n",
    "        self._env_shape = env_shape\n",
    "        self.position = np.array(init_position)\n",
    "        self._x_bounds = x_bounds\n",
    "        self._y_bounds = y_bounds\n",
    "        self._actions, self._traj = self.generate_trajectory()\n",
    "        self._found_fire = False\n",
    "        self._view = np.zeros((5,5))\n",
    "        \n",
    "        self._policy = torch.nn.Sequential(\n",
    "            torch.nn.Linear(25, 2048),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(2048, 2048),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(2048, 4)\n",
    "        )\n",
    "        \n",
    "        self._optimizer = torch.optim.Adam(self._policy.parameters())\n",
    "\n",
    "    def random_policy(self):\n",
    "        Q = np.random.rand(4)\n",
    "        return Q\n",
    "\n",
    "    def action(self, epsilon):\n",
    "        \"\"\"\n",
    "        sample and return action.\n",
    "        epsilon is the fraction of random actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # sample from trajectory (for debugging)\n",
    "        # action = self._actions.pop(0)\n",
    "\n",
    "        # sample randomly\n",
    "        Q_rand = self.random_policy()\n",
    "        \n",
    "        # sample form policy\n",
    "        view = self._view.flatten()\n",
    "        self._tensor_in = torch.tensor(view, dtype=torch.float32)\n",
    "        self._tensor_out = self._policy(self._tensor_in)\n",
    "        Q_policy = self._tensor_out.detach().numpy()\n",
    "        \n",
    "        Q = Q_rand if np.random.rand() < epsilon else Q_policy\n",
    "\n",
    "        viable = self.viable_actions()\n",
    "        Q[~viable] = - np.inf\n",
    "        action = np.argmax(Q)\n",
    "\n",
    "        delta = action2delta[action]\n",
    "        self.position += delta\n",
    "\n",
    "        return action\n",
    "\n",
    "    @property\n",
    "    def observation(self):\n",
    "        return self._view\n",
    "\n",
    "    @observation.setter\n",
    "    def observation(self, value):\n",
    "        self._view = value\n",
    "        \n",
    "    def backprop(self, reward):\n",
    "        self._optimizer.zero_grad()\n",
    "        \n",
    "        tensor_reward = torch.zeros(4, dtype=torch.float32)\n",
    "        tensor_reward[action] = reward\n",
    "        self._tensor_out.backward(tensor_reward)\n",
    "        \n",
    "        self._optimizer.step()\n",
    "        \n",
    "\n",
    "    def viable_actions(self):\n",
    "        \"\"\"\n",
    "        returns a mask\n",
    "        \"\"\"\n",
    "\n",
    "        viable = []\n",
    "        x, y = self.position\n",
    "\n",
    "        # check North\n",
    "        if y + 1 in self._y_bounds:\n",
    "            viable.append(\"N\")\n",
    "\n",
    "        # check South\n",
    "        if y - 1 in self._y_bounds:\n",
    "            viable.append(\"S\")\n",
    "\n",
    "        # check West\n",
    "        if x - 1 in self._x_bounds:\n",
    "            viable.append(\"W\")\n",
    "\n",
    "        # check East\n",
    "        if x + 1 in self._x_bounds:\n",
    "            viable.append(\"E\")\n",
    "\n",
    "        # convert to indices\n",
    "        viable = [name2action[v] for v in viable]\n",
    "\n",
    "        is_viable = np.zeros(4, dtype=np.bool)\n",
    "        is_viable[viable] = True\n",
    "\n",
    "        return is_viable\n",
    "\n",
    "    def reset(position=[0, 0]):\n",
    "        self.position = np.array(position)\n",
    "\n",
    "    def extinguish(self, env, range_xy=(3, 3)):\n",
    "        x, y = self.position\n",
    "        dx, dy = range_xy\n",
    "        env._state_map[y - dy : y + dy, x - dx : x + dx] = 0\n",
    "\n",
    "    def observe(self, env, fov=(3, 3)):\n",
    "        view = env._state_map\n",
    "        self._view = env._state_map[y - dy : y + dy, x - dx : x + dx]\n",
    "\n",
    "    def generate_trajectory(self, step_width=10):\n",
    "\n",
    "        actions = []\n",
    "        traj = []\n",
    "\n",
    "        for x_step in self._x_bounds[::10]:\n",
    "\n",
    "            midway = x_step + int(step_width / 2)\n",
    "\n",
    "            # go north\n",
    "            for y in self._y_bounds:\n",
    "                traj.append((x_step, y))\n",
    "                actions.append(name2action[\"N\"])\n",
    "\n",
    "            # go east\n",
    "            for x in range(x_step, midway):\n",
    "                traj.append((x, y))\n",
    "                actions.append(name2action[\"E\"])\n",
    "\n",
    "            # go down\n",
    "            for y in reversed(self._y_bounds):\n",
    "                traj.append((midway, y))\n",
    "                actions.append(name2action[\"S\"])\n",
    "\n",
    "            # go west\n",
    "            for x in range(midway, x_step + step_width):\n",
    "                traj.append((x, y))\n",
    "                actions.append(name2action[\"E\"])\n",
    "\n",
    "        return actions, traj\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_video(filename, frame_buffer):\n",
    "        writer = VideoWriter(filename, resolution=(500, 500), fps=60)\n",
    "        for frame in frame_buffer:\n",
    "            writer.write(np.array(frame)) \n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "drone = Agent(init_position=[10, 10])\n",
    "env = Environment(random_seed=260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-dab123dbe39b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "epsilon = range(0.9,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 200\n",
    "MAX_ITER = 1000\n",
    "EPSILON_SCHEDULE = np.linspace(0.9, 0.05, num=EPISODES)\n",
    "SAVE_VIDEO_EVERY = 1 #\n",
    "\n",
    "\n",
    "for episode, epsilon in zip(range(EPISODES), EPSILON_SCHEDULE):\n",
    "    \n",
    "    print(episode)\n",
    "    \n",
    "    env.reset()\n",
    "    drone.position = np.array([50,50])\n",
    "    \n",
    "    frame_buffer = []\n",
    "    \n",
    "    for it in range(MAX_ITER):\n",
    "        \n",
    "        action = drone.action(epsilon=0)\n",
    "        position = drone.position\n",
    "        drone.observation, reward = env.step(position, action, sim_step_every=30)\n",
    "\n",
    "        drone.backprop(reward)\n",
    "\n",
    "        frame_buffer.append(env.snapshot(drone.position))\n",
    "\n",
    "        if env.done: break\n",
    "            \n",
    "    if e % SAVE_VIDEO_EVERY == 0: drone.save_video(f\"episode-{episode}.mp4\", frame_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
